{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair\n",
    "import pprint\n",
    "import os, subprocess, sys,codecs, locale\n",
    "import re\n",
    "import traceback\n",
    "from collections import namedtuple\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pd.options.display.width = 250\n",
    "pd.options.display.max_colwidth = 250\n",
    "pd.options.display.max_rows = 250\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# some helper functions to obtain the right output from API\n",
    "def remove_spaces(input):\n",
    "    return input.replace(' ', '%20')\n",
    "\n",
    "def pdf_to_text(pdf_path):\n",
    "    #sys.setdefaultencoding(\"utf-8\")\n",
    "    os_encoding = locale.getpreferredencoding()\n",
    "    args = [\"pdftotext.exe\",pdf_path, \"-\"]\n",
    "    res = subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    output = (res.stdout).decode(os_encoding,'ignore')\n",
    "    return(output)\n",
    "\n",
    "def retrieve_and_save_pdf(id, overwrite=False):\n",
    "    if not os.path.exists(path + id + '.pdf') or overwrite:\n",
    "        base = 'https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/'\n",
    "        url = 'Document('+id+')/Resource'\n",
    "        end = ''\n",
    "        response = urllib.request.urlopen(remove_spaces(base+url))\n",
    "        data = response.read()      # a `bytes` object\n",
    "        f = open(path+id+'.pdf', 'w+b')\n",
    "        #binary_format = bytearray(byte_arr)\n",
    "        f.write(data)\n",
    "        f.close()\n",
    "    else:\n",
    "        pass\n",
    "        #print('file already downloaded')\n",
    "\n",
    "def query_API(skip):\n",
    "    base = 'https://gegevensmagazijn.tweedekamer.nl/OData/v4/2.0/'\n",
    "    url = \"/Zaak?$filter= Soort eq 'Motie'&$expand=Besluit($expand=Stemming),Document,ZaakActor,Agendapunt&$count=true&$skip=\"+skip\n",
    "    end = '&$format=application/json;odata.metadata=full'\n",
    "    response = urllib.request.urlopen(remove_spaces(base+url))\n",
    "    data = response.read()      # a `bytes` object\n",
    "    return(json.loads(data))\n",
    "#retrieve_and_save_pdf('1b7f8f8c-4579-4103-9f51-9280358f7b8a')\n",
    "#a = pdf_to_text(path+'3c2dbc59-beb1-42d8-9be4-e09238f20a97.pdf')\n",
    "#print(a)"
   ]
  },
  {
   "source": [
    "### Process API response to dict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def add_API_to_dict(data):\n",
    "    try:\n",
    "        for z in data['value']:\n",
    "            #print('\\n\\n','nummer',z['Nummer'])\n",
    "            info[z['Nummer']]={'Titel':z['Titel'],\n",
    "                              #'Status':z['Status'],\n",
    "                              'Onderwerp':z['Onderwerp'],\n",
    "                              'Vergaderjaar':z['Vergaderjaar'],\n",
    "                              'GestartOp':z['GestartOp']}\n",
    "\n",
    "            for b in z['Besluit']:\n",
    "                info[z['Nummer']]['BesluitTekst']=b['BesluitTekst']\n",
    "                info[z['Nummer']]['StemmingsSoort']=b['StemmingsSoort']\n",
    "                info[z['Nummer']]['BesluitSoort']=b['BesluitSoort']\n",
    "\n",
    "                #print('\\n','besluit',b['BesluitTekst'])\n",
    "                if b['BesluitSoort'] and b['BesluitSoort'] in ['Stemmen - aangenomen','Stemmen - verworpen', 'Stemmen - niet aangenomen']:        \n",
    "                    for s in b['Stemming']:\n",
    "                        #print(s)\n",
    "                        if s['Soort'] in ['Voor', 'Tegen']:\n",
    "                            info[z['Nummer']]['Stem_'+s['ActorFractie']]=(s['Soort'])\n",
    "                            info[z['Nummer']]['Aantal_stemmen_'+str(s['ActorFractie'])]=(s['FractieGrootte'])\n",
    "                        #info[z['Nummer']]['Vergissing_'+str(s['ActorFractie'])]=(s['Vergissing']) niet nodig, want Stem is altijd de gecorrigeerde stem (dus niet de vergissing)\n",
    "                    #print('stemmingenverwerkt')\n",
    "                    break\n",
    "            for a in z['Agendapunt']:\n",
    "                info[z['Nummer']]['AgendapuntOnderwerp']=a['Onderwerp']\n",
    "\n",
    "            for d in z['Document']:\n",
    "                if d['Soort'][:5]=='Motie':\n",
    "                    #print('\\n','doc',d)\n",
    "                    info[z['Nummer']]['doc_Id']=d['Id']\n",
    "                    info[z['Nummer']]['Volgnummer']=d['Volgnummer']\n",
    "                    info[z['Nummer']]['Datum']=d['Datum']\n",
    "                    if INCLUDE_PDF:\n",
    "                        #print('doing pdf work')\n",
    "                        retrieve_and_save_pdf(d['Id'])\n",
    "                        info[z['Nummer']]['Text']=pdf_to_text(path+d['Id']+'.pdf')\n",
    "            \n",
    "            info[z['Nummer']]['Indiener_persoon'] = []\n",
    "            info[z['Nummer']]['Medeindiener_persoon'] = []\n",
    "            for za in z['ZaakActor']:\n",
    "                #print('\\n','za',za)\n",
    "                if za['Relatie'] == 'Indiener' and za['ActorFractie']:\n",
    "                    info[z['Nummer']]['Indiener_persoon'].append(za['ActorNaam'])\n",
    "                    info[z['Nummer']]['Indiener_'+za['ActorFractie']]=1\n",
    "                \n",
    "                if za['Relatie'] == 'Medeindiener' and za['ActorFractie']:\n",
    "                    info[z['Nummer']]['Medeindiener_persoon'].append(za['ActorNaam'])\n",
    "                    info[z['Nummer']]['Medeindiener_'+za['ActorFractie']]=1\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        pprint.pprint(z) \n",
    "        traceback.print_exc() \n",
    "        print(s)\n",
    "        sys.exit()\n"
   ]
  },
  {
   "source": [
    "### Get all moties"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "started\n"
     ]
    }
   ],
   "source": [
    "# main routine to query api\n",
    "FIRST_ONLY = False\n",
    "INCLUDE_PDF = False\n",
    "START = 0\n",
    "MAXIMUM = 100000\n",
    "path = 'pdf/' # where to store pdfs of motions\n",
    "skip = 0\n",
    "info = {}\n",
    "print ('started')\n",
    "data = query_API(str(START))\n",
    "count = data['@odata.count']\n",
    "# add_API_to_dict(data)\n",
    "skip = START +  250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "query for  250\n",
      "query for  500\n",
      "query for  750\n",
      "query for  1000\n",
      "query for  1250\n",
      "query for  1500\n",
      "query for  1750\n",
      "query for  2000\n",
      "query for  2250\n",
      "query for  2500\n",
      "query for  2750\n",
      "query for  3000\n",
      "query for  3250\n",
      "query for  3500\n",
      "query for  3750\n",
      "query for  4000\n",
      "query for  4250\n",
      "query for  4500\n",
      "query for  4750\n",
      "query for  5000\n",
      "query for  5250\n",
      "query for  5500\n",
      "query for  5750\n",
      "query for  6000\n",
      "query for  6250\n",
      "query for  6500\n",
      "query for  6750\n",
      "query for  7000\n",
      "query for  7250\n",
      "query for  7500\n",
      "query for  7750\n",
      "query for  8000\n",
      "query for  8250\n",
      "query for  8500\n",
      "query for  8750\n",
      "query for  9000\n",
      "query for  9250\n",
      "query for  9500\n",
      "query for  9750\n",
      "query for  10000\n",
      "query for  10250\n",
      "query for  10500\n",
      "query for  10750\n",
      "query for  11000\n",
      "query for  11250\n",
      "query for  11500\n",
      "query for  11750\n",
      "query for  12000\n",
      "query for  12250\n",
      "query for  12500\n",
      "query for  12750\n",
      "query for  13000\n",
      "query for  13250\n",
      "query for  13500\n",
      "query for  13750\n",
      "query for  14000\n",
      "query for  14250\n",
      "query for  14500\n",
      "query for  14750\n",
      "query for  15000\n",
      "query for  15250\n",
      "query for  15500\n",
      "query for  15750\n",
      "query for  16000\n",
      "query for  16250\n",
      "query for  16500\n",
      "query for  16750\n",
      "query for  17000\n",
      "query for  17250\n",
      "query for  17500\n",
      "query for  17750\n",
      "query for  18000\n",
      "query for  18250\n",
      "query for  18500\n",
      "query for  18750\n",
      "query for  19000\n",
      "query for  19250\n",
      "query for  19500\n",
      "query for  19750\n",
      "query for  20000\n",
      "query for  20250\n",
      "query for  20500\n",
      "query for  20750\n",
      "query for  21000\n",
      "query for  21250\n",
      "query for  21500\n",
      "query for  21750\n",
      "query for  22000\n",
      "query for  22250\n",
      "query for  22500\n",
      "query for  22750\n",
      "query for  23000\n",
      "query for  23250\n",
      "query for  23500\n",
      "query for  23750\n",
      "query for  24000\n",
      "query for  24250\n",
      "query for  24500\n",
      "query for  24750\n",
      "query for  25000\n",
      "query for  25250\n",
      "query for  25500\n",
      "query for  25750\n",
      "query for  26000\n",
      "query for  26250\n",
      "query for  26500\n",
      "query for  26750\n",
      "query for  27000\n",
      "query for  27250\n",
      "query for  27500\n",
      "query for  27750\n",
      "query for  28000\n",
      "query for  28250\n",
      "query for  28500\n",
      "query for  28750\n",
      "query for  29000\n",
      "query for  29250\n",
      "query for  29500\n",
      "query for  29750\n",
      "query for  30000\n",
      "query for  30250\n",
      "query for  30500\n",
      "query for  30750\n",
      "query for  31000\n",
      "query for  31250\n",
      "query for  31500\n",
      "query for  31750\n",
      "query for  32000\n",
      "query for  32250\n",
      "query for  32500\n",
      "query for  32750\n",
      "query for  33000\n",
      "query for  33250\n",
      "query for  33500\n",
      "query for  33750\n",
      "query for  34000\n",
      "query for  34250\n",
      "query for  34500\n",
      "query for  34750\n",
      "query for  35000\n",
      "query for  35250\n",
      "query for  35500\n",
      "query for  35750\n",
      "query for  36000\n",
      "query for  36250\n",
      "query for  36500\n",
      "query for  36750\n",
      "query for  37000\n",
      "query for  37250\n",
      "query for  37500\n",
      "query for  37750\n",
      "query for  38000\n",
      "query for  38250\n",
      "query for  38500\n",
      "query for  38750\n",
      "query for  39000\n",
      "query for  39250\n",
      "query for  39500\n",
      "query for  39750\n",
      "query for  40000\n",
      "query for  40250\n",
      "query for  40500\n",
      "query for  40750\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "if not FIRST_ONLY:\n",
    "    while skip < count + 250 and skip < MAXIMUM:\n",
    "        print('query for ',skip)\n",
    "        data = query_API(str(skip))\n",
    "        add_API_to_dict(data)\n",
    "        skip +=250\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('moties_unprocessed.pickle', 'wb') as handle:\n",
    "    pickle.dump(info, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "source": [
    "## Download PDFs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"moties_unprocessed.pickle\",\"rb\")\n",
    "info = pickle.load(file)\n",
    "\n",
    "download_errors = set()\n",
    "no_doc_id = set()\n",
    "\n",
    "for i, (k,v) in enumerate(info.items()):\n",
    "    if i % 100 == 0: print(i)\n",
    "    doc_id = v.get('doc_Id', None)\n",
    "    if doc_id:\n",
    "        try:\n",
    "            retrieve_and_save_pdf(doc_id)\n",
    "            info[k]['Text'] = pdf_to_text(path + doc_id +'.pdf')\n",
    "        except Exception as e:\n",
    "            print('download or pdf_to_text error')\n",
    "            traceback.print_exc() \n",
    "            download_errors.add((k,doc_id))\n",
    "    else:\n",
    "        no_doc_id.add(k)\n",
    "df_text = pd.DataFrame(info).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some pdf download may have been corrupted. By running this function, we try to redownload the pdf and update the text. Unfortunatly it seems the pdfs are simply corrupted\n",
    "no_text_index = df_text[(df['Text']=='') | (df_text['Text']=='nan')].index\n",
    "print(len(no_text_index), 'no texts found')\n",
    "texts = []\n",
    "for index in no_text:\n",
    "    doc_Id = df_text.loc[no_text[0],'doc_Id']\n",
    "    retrieve_and_save_pdf(doc_Id, overwrite=True)\n",
    "    t = pdf_to_text(path + doc_Id + '.pdf')\n",
    "    if t != '':\n",
    "        df_text.loc[no_text[0],'Text'] = t\n",
    "        texts.append(t)\n",
    "print(len(texts), 'new pdfs found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('df_text_before_preprocessing', 'wb') as handle:\n",
    "    pickle.dump(df_text, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "source": [
    "### Make dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open(\"df_text_before_preprocessing\",\"rb\")\n",
    "df = pickle.load(file)\n",
    "print(len(df))\n",
    "column_list = df.columns.values # current columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].astype(str)\n",
    "df['Text'] = df['Text'].str.replace('\\xad', '')\n",
    "df['Text'] = df['Text'].str.replace('\\n', '')\n",
    "df['Text'] = df['Text'].str.replace('\\r', '')\n",
    "df['Text'] = df['Text'].str.replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sum(df['doc_Id'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def preprocessing_motie(text,doc_id,i): # remove start and end of document to only include relevant text\n",
    "    try:\n",
    "        if text and doc_id and text != 'nan':\n",
    "            try:\n",
    "                regex = re.findall(r\"\\d{4}(.*?)Nr.*gehoord de beraadslaging(.*)\",text,re.DOTALL)\n",
    "                regex = (' '.join(regex[0]))\n",
    "            except Exception:\n",
    "                regex = re.findall(r\"\\d{4}(.*?)\",text,re.DOTALL)\n",
    "                regex = (' '.join(regex[0]))\n",
    "            return regex\n",
    "        else:\n",
    "            if not doc_id:\n",
    "                doc_id.append(i)\n",
    "            elif not text:\n",
    "                no_text.append(i)\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        print(traceback.print_exc())\n",
    "        error_processing.append(i)\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "no_text = []\n",
    "error_processing = []\n",
    "doc_id = []\n",
    "df['Text_processed'] = np.vectorize(preprocessing_motie)(df['Text'],df['doc_Id'],df.index)\n",
    "len(no_text),len(error_processing),len(doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#verwijder hoofdelijke stemmingen\n",
    "print (len(df), 'removing hoofdelijke stemmingen')\n",
    "df.drop(df[df['StemmingsSoort'] == 'Hoofdelijk'].index, inplace=True)\n",
    "print (len(df), '\\n')\n",
    "\n",
    "#verwijder moties zonder stemming\n",
    "print (len(df), 'removing moties without vote')\n",
    "print(df['BesluitSoort'].value_counts())\n",
    "df = df[((df['BesluitSoort'] == 'Stemmen - aangenomen') | (df['BesluitSoort'] == 'Stemmen - verworpen'))]\n",
    "print (len(df), '\\n')\n",
    "\n",
    "#recode besluitsoort naar -1 en 1\n",
    "print(df['BesluitSoort'].value_counts())\n",
    "df['BesluitSoort']=df['BesluitSoort'].replace({'Stemmen - verworpen':'0','Stemmen - aangenomen':'1'})\n",
    "df['BesluitSoort']=pd.to_numeric(df['BesluitSoort'])\n",
    "print(df['BesluitSoort'].value_counts(), '\\n')\n",
    "\n",
    "#recode voor en tegen naar 1 en -1\n",
    "stem_column = [c for c in column_list if 'Stem_' in c]\n",
    "print(df['Stem_50PLUS'].value_counts())\n",
    "df[stem_column]=df[stem_column].replace({'Tegen':'-1','Voor':'1','Niet deelgenomen':np.nan})\n",
    "print(df['Stem_50PLUS'].value_counts(), '\\n')\n",
    "\n",
    "#cast to datetime and sort old to new\n",
    "df['Datum'] = pd.to_datetime(df['Datum'], utc=True)\n",
    "df['GestartOp'] = pd.to_datetime(df['GestartOp'], utc=True)\n",
    "df['Datum'] = df['Datum'].dt.tz_localize(None)\n",
    "df['GestartOp'] = df['GestartOp'].dt.tz_localize(None)\n",
    "df.sort_values('Datum',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#bereken voor en tegenstemmen\n",
    "aantal_stemmen_column = [c for c in column_list if 'Aantal' in c]\n",
    "\n",
    "for i in range(len(stem_column)):\n",
    "    df[stem_column[i]]=pd.to_numeric(df[stem_column[i]])\n",
    "    df[aantal_stemmen_column[i]]=pd.to_numeric(df[aantal_stemmen_column[i]])\n",
    "\n",
    "res = np.multiply(df[stem_column],df[aantal_stemmen_column])\n",
    "voor = res[res > 0].sum(axis=1)\n",
    "tegen = abs(res[res < 0].sum(axis=1))\n",
    "df['Voor'], df['Tegen'] = voor,tegen\n",
    "df['Delta'] = abs(df['Voor']- df['Tegen'])\n",
    "df['Sum'] = (df['Voor']+ df['Tegen'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "df = df.replace(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Jaar'] = df['Datum'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 26 moties for first year, so let's chop\n",
    "df = df[(df['Jaar'] > 2008) & (df['Jaar'] < 2021)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import datetime\n",
    "Cabinet = namedtuple('Cabinet', ['name','start', 'end', 'demissionair','coalition'])\n",
    "Chamber = namedtuple('Chamber', ['name','start', 'end', 'numyears'])\n",
    "\n",
    "chamber_input = [\n",
    "    \"Balkenende IV,01 01 2009,16 06 2010\",# mark begin 2008 is not correct (should be 30 11 2006) but for dataset purpose it it\n",
    "    \"Rutte I,17 06 2010,19 09 2012\",\n",
    "    \"Rutte II,20 09 2012,22 03 2017\",\n",
    "    \"Rutte III,23 03 2017,31 12 2020\"] # mark end 2020 is not correct but for dataset purpose it it\n",
    "\n",
    "chambers = {}\n",
    "for c in chamber_input:\n",
    "    c = c.split(',')\n",
    "    name = c[0]\n",
    "    start = list(map(int, c[1].split()))\n",
    "    startdate = datetime.datetime(start[2], start[1], start[0])\n",
    "    end = list(map(int, c[2].split()))\n",
    "    enddate = datetime.datetime(end[2], end[1], end[0])\n",
    "    numyears = (enddate - startdate).days/365\n",
    "    chambers[name] = Chamber(name, startdate, enddate, numyears)\n",
    "\n",
    "\n",
    "cabinets_input = [\n",
    "    # \"Balkenende III,07 07 2006,22 02 2007,CDA VVD\",\n",
    "    # \"Balkenende IV,22 02 2007,14 10 2010,CDA PvdA ChristenUnie\",\n",
    "    \"Balkenende IV,18 09 2008,13 10 2010,20 02 2010,CDA PvdA ChristenUnie\",\n",
    "    \"Rutte I,14 10 2010,4 11 2012,23 04 2012,VVD CDA PVV\",\n",
    "    \"Rutte II,5 11 2012,25 10 2017,14 03 2017,VVD PvdA\",\n",
    "    \"Rutte III,26 10 2017,15 01 2022,15 01 2022,CDA VVD D66 ChristenUnie\"]\n",
    "\n",
    "cabinets = {}\n",
    "for c in cabinets_input:\n",
    "    c = c.split(',')\n",
    "    name = c[0]\n",
    "    start = list(map(int, c[1].split()))\n",
    "    startdate = datetime.datetime(start[2], start[1], start[0])\n",
    "    end = list(map(int, c[2].split()))\n",
    "    enddate = datetime.datetime(end[2], end[1], end[0])\n",
    "    demissionair = list(map(int, c[3].split()))\n",
    "    demissionair_date = datetime.datetime(demissionair[2], demissionair[1], demissionair[0])\n",
    "    coalition = tuple(c[4].split())\n",
    "    cabinets[name] = Cabinet(name, startdate, enddate, demissionair_date, coalition)\n",
    "cabinets, chambers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def get_period(entities, date):\n",
    "    # date = datetime.datetime.strptime(date_string[:10], '%Y-%m-%d')\n",
    "    # date = date.replace(tzinfo=None)\n",
    "    for c in entities.values():\n",
    "        if ((c.start <= date) or (date < datetime.datetime(2009,1,1))) and c.end >= date:\n",
    "            return c.name\n",
    "    else:\n",
    "        return 'Error'\n",
    "\n",
    "\n",
    "def get_demissionair(cabinets, date):\n",
    "    # date = datetime.datetime.strptime(date_string[:10], '%Y-%m-%d')\n",
    "    c = get_period(cabinets, date)\n",
    "    # date = date.replace(tzinfo=None)\n",
    "\n",
    "    return False if date < cabinets[c].demissionair else True\n",
    "    \n",
    "df['Kabinet'] = [get_period(cabinets, d) for d in df['Datum']]\n",
    "df['Demissionair'] = [get_demissionair(cabinets, d) for d in df['Datum']]\n",
    "df['Kamer'] = [get_period(chambers, d) for d in df['Datum']]\n",
    "df['Kabinet'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_PDF = True\n",
    "\n",
    "# sort columns\n",
    "column_list = df.columns.values # current columns\n",
    "column_order = ['Titel','AgendapuntOnderwerp','Onderwerp','Datum','GestartOp','Vergaderjaar','Jaar','Kamer', 'Kabinet','Demissionair','StemmingsSoort','BesluitSoort','BesluitTekst','doc_Id','Volgnummer', 'Voor', 'Tegen', 'Delta', 'Sum', 'Text', 'Text_processed'] # desired columns\n",
    "\n",
    "not_in_column_order = set(column_list) - set(column_order)\n",
    " # don't throw away columns\n",
    "column_order += sorted(not_in_column_order)\n",
    "df = df[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-005f052dbe92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Vergaderjaar'\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m'2006-2007'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#df = df[df['Onderwerp'].str.contains('klimaat')]\n",
    "import pickle\n",
    "with open('moties_processed_df.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}