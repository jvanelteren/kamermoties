{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from top2vec import Top2Vec\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('./data')\n",
    "def load(filename):\n",
    "    f = open(DATA_DIR/filename,\"rb\")\n",
    "    return pickle.load(f)\n",
    "    \n",
    "def save(data, filename):\n",
    "    with open(DATA_DIR/filename, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_voor_tegen_index(df):\n",
    "    print(len(df))\n",
    "    stem_column = [c for c in df.columns if 'Stem_' in c and c != 'Stem_persoon']\n",
    "    stem_column_adj = [c[5:] for c in df.columns if 'Stem_' in c and c != 'Stem_persoon']\n",
    "    stem_array = df[stem_column].values.tolist()\n",
    "    assert len(stem_array[0]) == len(stem_column)\n",
    "    voor = [[stem_column_adj[i] for i, stem in enumerate(motie) if stem == 1] for motie in stem_array]\n",
    "    tegen = [[stem_column_adj[i] for i, stem in enumerate(motie) if stem == 0] for motie in stem_array]\n",
    "    df['Partijen_Voor'] = voor\n",
    "    df['Partijen_Tegen'] = tegen\n",
    "    df['Index']=list(range(len(df)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before removal empty texts 29514\n",
      "after removal empty texts 29484\n",
      "29484\n",
      "ipykernel_launcher:12: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file = open(\"moties_processed_df.pickle\",\"rb\")\n",
    "df = pickle.load(file)\n",
    "print('before removal empty texts',len(df))\n",
    "\n",
    "# remove moties without text\n",
    "mask = (df['Text']=='') | (df['Text'].isna())\n",
    "df = df.loc[~mask]\n",
    "print('after removal empty texts',len(df))\n",
    "\n",
    "df = add_voor_tegen_index(df)\n",
    "documents = df['Text'].values\n",
    "df['BesluitTekst'] = df['BesluitTekst'].str.replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-546989b5ddbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msentence_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrip_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mbigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbigram_phraser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhraser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mindieners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mindiener\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindiener\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Indiener_persoon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mindiener\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, phrases_model)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'source_vocab length %i'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrases_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mphrases_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexport_phrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_tuples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbigram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphrasegrams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Phraser repeat %s'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36mexport_phrases\u001b[1;34m(self, sentences, out_delimiter, as_tuples)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;31m# keeps only not None scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mas_tuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m                     \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mbigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyze_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;31m# keeps only not None scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m             \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mas_tuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\gensim\\models\\phrases.py\u001b[0m in \u001b[0;36manalyze_sentence\u001b[1;34m(self, sentence, threshold, common_terms, scorer)\u001b[0m\n\u001b[0;32m    196\u001b[0m                     \u001b[1;31m# release words individually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlast_uncommon\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_between\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m                         \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m                     \u001b[0min_between\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                     \u001b[0mlast_uncommon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# got the code for making the bigram part from https://lppier.github.io/\n",
    "sentence_stream = [simple_preprocess(strip_tags(doc), deacc=True) for doc in documents]\n",
    "bigram = Phrases(sentence_stream, min_count=10)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "indieners = {indiener[-1].lower() for indiener in df['Indiener_persoon'].str.split() if indiener}\n",
    "years = {word for doc in sentence_stream for word in doc if word.startswith('x') and not word.startswith('xin')}\n",
    "manual_stopwords = {'faber', 'lacin', 'kroger', 'iv', 'beschikt', 'die', 'vaststelling', 'lid','vi', 'viii', 'iii', 'iv', 'ii', 'i'}\n",
    "stopwords = indieners | years | manual_stopwords\n",
    "   \n",
    "def bigram_stopword_preprocess(doc):\n",
    "    sentence_stream = simple_preprocess(strip_tags(doc), deacc=True)\n",
    "    sentence_stream = [word for word in sentence_stream if word not in stopwords]\n",
    "    return bigram_phraser[sentence_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Top2Vec(documents, speed='deep-learn', tokenizer=bigram_stopword_preprocess, workers=4)\n",
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"data/doc2vec_deep_bigram_enhanced_stopwords\")"
   ]
  },
  {
   "source": [
    "## Reduce topic number to something more manageable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_topics(num_topics):\n",
    "    print(f'performing reduction to {num_topics} topics')\n",
    "    reduced_topics = model.hierarchical_topic_reduction(num_topics)\n",
    "    topic_words, word_scores, topic_nums = model.get_topics(reduced=True)\n",
    "    reduced_topics = tuple(tuple(sorted(t)) for t in reduced_topics)\n",
    "    return reduced_topics, topic_words\n",
    "\n",
    "def find_diff(reduced1, reduced2, reverse=False):\n",
    "    # find the topics that where merged and return their index\n",
    "    if not reverse:\n",
    "        changed = set(reduced1) - set(reduced2)\n",
    "        return [index for index, topic in enumerate(reduced1) if topic in changed]\n",
    "    else:\n",
    "        changed = set(reduced2) - set(reduced1)\n",
    "        return [index for index, topic in enumerate(reduced2) if topic in changed]\n",
    "\n",
    "\n",
    "def print_merge(large, small, num_words=50):\n",
    "    print(f'\\ninspecting difference from {len(large[0])} to {len(small[0])} topics')\n",
    "    print('old topics')\n",
    "    for i in find_diff(large[0],small[0]):\n",
    "        print(large[1][i][:num_words])\n",
    "    print('new topic')\n",
    "    for i in find_diff(large[0],small[0], reverse=True):\n",
    "        print(small[1][i][:num_words])\n",
    "\n",
    "def find_optimal_num_topics():\n",
    "    start = 17\n",
    "    stop = 6\n",
    "    reductions = {i: get_reduced_topics(i) for i in range(start, stop, -1)}\n",
    "    for i in range(start, stop +1,-1):\n",
    "        print(i)\n",
    "        print_merge(reductions[i], reductions[i-1])\n",
    "# find_optimal_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a choice to how many topics to reduce\n",
    "num_topics = 16\n",
    "reduced_topics = model.hierarchical_topic_reduction(num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect number of documents for each topic\n",
    "# topic_sizes, topic_nums = model.get_topic_sizes(reduced=True)\n",
    "# topic_words, word_scores, topic_nums = model.get_topics(reduced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "  0: 'Onderwijs',  \n",
    "  1: 'Buitenlandse zaken',  \n",
    "  2: 'Algemene zaken',  \n",
    "  3: 'Natuur & gaswinning',  \n",
    "  4: 'Landbouw & dierenwelzijn',  \n",
    "  5: 'Zorg',  \n",
    "  6: 'Sociale zaken',  \n",
    "  7: 'Justitie',  \n",
    "  8: 'Pensioenstelsel',  \n",
    "  9: 'Europese Unie',  \n",
    "  10: 'Klimaat & energie',  \n",
    "  11: 'Milieu & regelgeving',  \n",
    "  12: 'Zorg',  \n",
    "  13: 'Openbaar vervoer',  \n",
    "  14: 'Financiele sector',  \n",
    "  15: 'Wonen'\n",
    "  }\n",
    "\n",
    "doc_ids = list(range(len(documents)))\n",
    "topic_nums, topic_score, topic_words, word_scores = model.get_documents_topics(doc_ids,reduced=True)\n",
    "topic_names = [topics[t] for t in topic_nums]\n",
    "assert len(topic_nums) == len(df)\n",
    "df['Topic'] = topic_names\n",
    "topic_nums, topic_score, topic_words, word_scores = model.get_documents_topics(doc_ids,reduced=False)\n",
    "df['Topic_initial'] = topic_nums\n",
    "df['Topic_score'] = topic_score\n",
    "# df.sort_values(['Topic_initial', 'Topic_score'], ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "sum(df['Topic_initial']==3)"
   ]
  },
  {
   "source": [
    "## Optional add climate deepdive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes(reduced=False)\n",
    "topic_words, word_scores, topic_nums = model.get_topics(reduced=False)\n",
    "climate_idx = [topic_nums for t in topic_nums if t in reduced_topics[10]]\n",
    "climate_subtopics = {128: 'Afhankelijkheid fossiele brandstoffen',\n",
    " 165: 'CO2 reductie',\n",
    " 5: 'Voldoen aan Parijs',\n",
    " 141: 'Electriciteit',\n",
    " 126: 'Groningen',\n",
    " 205: 'Zonnepanelen',\n",
    " 96: 'Energierekening betalen',\n",
    " 236: 'Biomassa',\n",
    " 105: 'Kolencentrales',\n",
    " 239: 'Windturbines - overlast',\n",
    " 29: 'Windturbines - subsidie'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Klimaat'] = df.loc[df['Topic']=='Klimaat & energie', 'Topic_initial'].map(climate_subtopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df[:1000], 'df_including_topics.pickle')\n",
    "save(df, 'df_including_topics_full.pickle')\n"
   ]
  },
  {
   "source": [
    "# Prepare slimmed down versions for production"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:2: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "29484\n",
      "11448\n"
     ]
    }
   ],
   "source": [
    "df = load(\"df_including_topics_full.pickle\")\n",
    "df['BesluitTekst'] = df['BesluitTekst'].str.replace('.','')\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "reduced_topics = model.hierarchical_topic_reduction(100)\n",
    "\n",
    "doc_ids = list(range(len(df)))\n",
    "# don't sort the df before this operation\n",
    "topic_nums, topic_score, topic_words, word_scores = model.get_documents_topics(doc_ids,reduced=True)\n",
    "topics = [t[0] for t in topic_words]\n",
    "\n",
    "df['Topic_initial'] = topics\n",
    "df['Topic_score'] = topic_score\n",
    "df.sort_values(['Topic_initial', 'Topic_score'], ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df = df[df['Kamer']=='Rutte III']\n",
    "print(len(df))\n",
    "# important do this only after all row filters have been set\n",
    "df.index = df['Index']\n",
    "\n",
    "\n",
    "stem_column = [c for c in df.columns if 'Stem_' in c and c != 'Stem_persoon']\n",
    "required_cols = ['Kamer', 'Jaar','Indienende_partij', 'BesluitSoort','BesluitTekst','Topic_initial', 'Topic_score','Indienende_persoon_partij','Partijen_Voor', 'Partijen_Tegen', 'Text']\n",
    "\n",
    "# streamlit has problems with category type: \n",
    "# https://github.com/streamlit/streamlit/issues/47\n",
    "# for col in stem_column + required_cols[:-4]:\n",
    "#     df[col] = df[col].astype('category')\n",
    "\n",
    "df = df[stem_column + required_cols]\n",
    "save(df, \"df_production.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_documents(self, doc_ids):\n",
    "    \"\"\"\n",
    "    Delete documents from current model.\n",
    "    Warning: If document ids were not used in original model, deleting\n",
    "    documents will change the indexes and therefore doc_ids.\n",
    "    The documents will be deleted from the current model without changing\n",
    "    existing document, word and topic vectors. Topic sizes will be updated.\n",
    "    If deleting a large quantity of documents relative to the current model\n",
    "    size a new model should be trained for best results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_ids: List of str, int\n",
    "        A unique value per document that is used for referring to documents\n",
    "        in search results.\n",
    "    \"\"\"\n",
    "    # make sure documents exist\n",
    "    self._validate_doc_ids(doc_ids, doc_ids_neg=[])\n",
    "\n",
    "    # update index\n",
    "    if self.documents_indexed:\n",
    "        # delete doc_ids from index\n",
    "        index_ids = [self.doc_id2index_id(doc_id) for doc_id in doc_ids]\n",
    "        for index_id in index_ids:\n",
    "            self.document_index.mark_deleted(index_id)\n",
    "        # update index_id and doc_ids\n",
    "        for doc_id in doc_ids:\n",
    "            self.doc_id2index_id.pop(doc_id)\n",
    "        for index_id in index_ids:\n",
    "            self.index_id2doc_id.pop(index_id)\n",
    "\n",
    "    # get document indexes from ids\n",
    "    doc_indexes = self._get_document_indexes(doc_ids)\n",
    "\n",
    "    # delete documents\n",
    "    if self.documents is not None:\n",
    "        self.documents = np.delete(self.documents, doc_indexes, 0)\n",
    "\n",
    "    # delete document ids\n",
    "    if self.document_ids is not None:\n",
    "        for doc_id in doc_ids:\n",
    "            self.doc_id2index.pop(doc_id)\n",
    "        keys = list(self.doc_id2index.keys())\n",
    "        self.document_ids = np.array(keys)\n",
    "        values = list(range(0, len(self.doc_id2index.values())))\n",
    "        self.doc_id2index = dict(zip(keys, values))\n",
    "\n",
    "    # delete document vectors\n",
    "    self._set_document_vectors(np.delete(self._get_document_vectors(norm=False), doc_indexes, 0))\n",
    "\n",
    "    if self.embedding_model == 'doc2vec':\n",
    "        num_docs = len(doc_indexes)\n",
    "        self.model.docvecs.count -= num_docs\n",
    "        self.model.docvecs.max_rawint -= num_docs\n",
    "        self.model.docvecs.vectors_docs_norm = None\n",
    "        self.model.docvecs.init_sims()\n",
    "\n",
    "    # update topics\n",
    "    # self._unassign_documents_from_topic(doc_indexes, hierarchy=False)\n",
    "\n",
    "    if self.hierarchy is not None:\n",
    "        self._unassign_documents_from_topic(doc_indexes, hierarchy=True)\n",
    "\n",
    "model.delete_documents = delete_documents\n",
    "df = load(\"df_including_topics_full.pickle\")\n",
    "model.delete_documents(model,list(range(len(df))))\n",
    "model.delete_documents = 1\n",
    "model.save(\"data/doc2vec_production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 5, 96, 25, 24], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "df = load(\"df_production.pickle\")\n",
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords= ['klimaat'] , num_topics=4)\n",
    "topic_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11448"
      ]
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "df = load(\"df_production.pickle\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2\\nTweede Kamer der Staten-Generaal\\nVergaderjaar 2018–2019 \\n21 501-32 Landbouw- en Visserijraad \\nNr. 1158   GEWIJZIGDE MOTIE VAN HET LID OUWEHAND TER VERVANGING \\nVAN DIE GEDRUKT ONDER NR. 1154 \\nVoorgesteld 24 januari 2019 \\nDe Kamer, \\ngehoord de beraadslaging, \\nverzoekt de regering, een verbod in te stellen op alle kooihuisvesting van \\ndieren in de veehouderij, \\nen gaat over tot de orde van de dag. \\nOuwehand\\n \\n \\n \\n \\nkst-21501-32-1158\\nISSN 0921 - 7371\\n’s-Gravenhage 2019 Tweede Kamer, vergaderjaar 2018–2019, 21 501-32, nr. 1158'"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "df[(df['Topic_initial']=='dieren') & (df['Kamer']=='Rutte III')]['Text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29484\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Verworpen.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              11499\n",
       "Aangenomen.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             10110\n",
       "Verworpen                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                4647\n",
       "Aangenomen                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               3226\n",
       "Verworpen.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  1\n",
       "Aangenomen. (Mededeling Voorzitter: Voorts deel ik u mede dat op 20 mei jl. de motie-Van Klaveren over een algemene daadkrachtige aanpak van ziekenhuizen (Kamerstuk 33077, nr. 13) door de Kamer is aangenomen. Bij controle van de stemmingsuitslagen is echter gebleken dat 74 leden vóór deze motie hebben gestemd en 76 tegen. Omdat ingevolge artikel 71, tweede lid van het Reglement van Orde geen wijziging meer in de uitslag kan worden aangebracht kan ik mij slechts beperken tot deze aantekening in de Handelingen. )        1\n",
       "Name: BesluitTekst, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "df = load(\"df_including_topics_full.pickle\")\n",
    "print(len(df))\n",
    "df['BesluitTekst'].value_counts()"
   ]
  },
  {
   "source": [
    "## Backup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary = load(\"df_production.pickle\")\n",
    "necessary_idx = set(necessary.index)\n",
    "\n",
    "full = load(\"df_including_topics_full.pickle\")\n",
    "full_idx = set(full['Index'])\n",
    "\n",
    "redundant_idx = full_idx - necessary_idx\n",
    "assert len(necessary_idx) + len(redundant_idx) == len(full_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0], dtype=int64),\n",
       " array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246],\n",
       "       dtype=int64))"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "model.get_topic_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([22728,  9991,  5958,  2969,  6021,  8364, 19787,  8385,  8357,\n",
       "       16140])"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num =3, num_docs= 100)\n",
    "document_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_3 = full[(full['Topic_initial']==3)&(full['Index'].isin(redundant_idx))]['Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "len(list(to_remove_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[531 458 445 404 401 350 347 333 326 312 309 308 301 287 285 278 276 273\n",
      " 270 265 263 257 255 240 236 230 230 228 227 222 220 211 205 205 203 199\n",
      " 197 195 193 192 190 182 181 180 175 174 171 169 166 165 164 163 161 159\n",
      " 159 159 158 156 155 154 154 151 151 150 150 148 147 143 141 141 136 134\n",
      " 133 133 132 131 129 127 126 125 124 124 122 121 120 118 118 118 117 116\n",
      " 114 113 112 110 109 108 108 107 107 105 105 103 103 103 102 100  99  98\n",
      "  98  97  96  96  96  96  96  96  95  95  95  94  93  91  91  91  89  88\n",
      "  88  88  87  87  86  86  86  85  85  84  84  84  83  83  81  81  81  81\n",
      "  80  80  80  80  79  79  79  79  79  79  78  77  76  75  75  75  75  74\n",
      "  73  72  72  71  71  70  70  69  69  68  68  68  67  67  67  67  65  65\n",
      "  65  64  63  62  62  61  60  60  59  59  58  58  57  57  57  56  56  56\n",
      "  56  55  55  55  54  54  53  52  52  52  52  52  50  50  49  48  47  47\n",
      "  46  46  46  45  45  45  45  45  45  44  44  43  43  42  41  41  40  40\n",
      "  39  38  38  38  37  36  35  34  34  33  33  32  23]\n",
      "[531 458 445 401 350 347 333 326 312 309 308 301 287 285 278 276 273 270\n",
      " 265 263 257 255 240 236 230 230 228 227 222 220 211 205 205 203 199 197\n",
      " 195 193 192 190 182 181 180 175 174 171 169 166 165 164 163 161 159 159\n",
      " 159 158 156 155 154 154 151 151 150 150 148 147 143 141 141 136 134 133\n",
      " 133 132 131 129 127 126 126 125 124 124 122 121 120 118 118 118 117 116\n",
      " 114 113 112 110 109 108 108 107 107 105 105 103 103 103 102 100  99  98\n",
      "  98  97  96  96  96  96  96  96  95  95  95  94  93  91  91  91  89  88\n",
      "  88  88  87  87  86  86  86  85  85  84  84  84  83  83  81  81  81  81\n",
      "  80  80  80  80  79  79  79  79  79  79  78  77  76  75  75  75  75  74\n",
      "  73  72  72  71  71  70  70  69  69  68  68  68  67  67  67  67  65  65\n",
      "  65  64  63  62  62  61  60  60  59  59  58  58  57  57  57  56  56  56\n",
      "  56  55  55  55  54  54  53  52  52  52  52  52  50  50  49  48  47  47\n",
      "  46  46  46  45  45  45  45  45  45  44  44  43  43  42  41  41  40  40\n",
      "  39  38  38  38  37  36  35  34  34  33  33  32  23]\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "print(model.get_topic_sizes()[0])\n",
    "model.delete_documents(list(to_remove_3))\n",
    "print(model.get_topic_sizes()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Invalid number of documents: original topic 3 only has 0 documents.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-1cb2ef9b984a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdocument_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_documents_by_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_num\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m404\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_documents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36msearch_documents_by_topic\u001b[1;34m(self, topic_num, num_docs, return_documents, reduced)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_topic_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1738\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_topic_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1740\u001b[0m             \u001b[0mtopic_document_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc_top\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtopic_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36m_validate_topic_search\u001b[1;34m(self, topic_num, num_docs, reduced)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnum_docs\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopic_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                 raise ValueError(f\"Invalid number of documents: original topic {topic_num}\"\n\u001b[0m\u001b[0;32m    872\u001b[0m                                  f\" only has {self.topic_sizes[topic_num]} documents.\")\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid number of documents: original topic 3 only has 0 documents."
     ]
    }
   ],
   "source": [
    "# model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "document_scores, document_ids = model.search_documents_by_topic(topic_num =3, num_docs= 404,return_documents=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29484"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "sums, _ = model.get_topic_sizes()\n",
    "sum(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2\\nTweede Kamer der Staten-Generaal\\nVergaderjaar 2019–2020 \\n35 420 Noodpakket banen en economie \\nNr. 66   MOTIE VAN DE LEDEN BAUDET EN VAN HAGA \\nVoorgesteld 28 mei 2020 \\nDe Kamer, \\ngehoord de beraadslaging, \\nconstaterende dat in 2019 de Klimaatwet in werking is getreden; \\noverwegende dat de kosten van het Nederlandse klimaatbeleid naar \\nverwachting zullen oplopen tot in totaal 1.000 miljard euro; \\noverwegende dat het Nederlandse klimaatbeleid, zelfs als de theorie over \\nopwarming van de aarde door toedoen van de mens zou kloppen, die \\nopwarming met hoogstens 0,00007 graden zou beperken; \\noverwegende dat de kosten van het klimaatbeleid in geen enkele \\nverhouding staan tot de opbrengsten; \\nroept de regering op, om het klimaatbeleid te staken en het geld dat \\nhierdoor bespaard wordt in te zetten om de klappen van de coronacrisis \\nop te vangen, \\nen gaat over tot de orde van de dag. \\nBaudet \\nVan Haga\\n \\n \\n \\n \\nkst-35420-66\\nISSN 0921 - 7371\\n’s-Gravenhage 2020 Tweede Kamer, vergaderjaar 2019–2020, 35 420, nr. 66'"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "len(necessary[necessary['Topic_initial']==3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load('df_production.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([2020, 2019, 2018, 2017], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df['Jaar'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['2017', '2018', '2019', '2020']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}