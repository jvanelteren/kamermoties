{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from top2vec import Top2Vec\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import strip_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('./data')\n",
    "def load(filename):\n",
    "    f = open(DATA_DIR/filename,\"rb\")\n",
    "    return pickle.load(f)\n",
    "    \n",
    "def save(data, filename):\n",
    "    with open(DATA_DIR/filename, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before removal empty texts 29514\n",
      "after removal empty texts 29484\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file = open(\"moties_processed_df.pickle\",\"rb\")\n",
    "df = pickle.load(file)\n",
    "print('before removal empty texts',len(df))\n",
    "\n",
    "# remove moties without text\n",
    "mask = (df['Text']=='') | (df['Text'].isna())\n",
    "df = df.loc[~mask]\n",
    "print('after removal empty texts',len(df))\n",
    "\n",
    "documents = df['Text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "29484\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "stem_column = [c for c in df.columns if 'Stem_' in c and c != 'Stem_persoon']\n",
    "stem_column_adj = [c[5:] for c in df.columns if 'Stem_' in c and c != 'Stem_persoon']\n",
    "stem_array = df[stem_column].values.tolist()\n",
    "assert len(stem_array[0]) == len(stem_column)\n",
    "voor = [[stem_column_adj[i] for i, stem in enumerate(motie) if stem == 1] for motie in stem_array]\n",
    "tegen = [[stem_column_adj[i] for i, stem in enumerate(motie) if stem == 0] for motie in stem_array]\n",
    "df['Partijen_Voor'] = voor\n",
    "df['Partijen_Tegen'] = tegen\n",
    "df['Index']=list(range(len(df)))"
   ]
  },
  {
   "source": [
    "## Model Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# got the code for making the bigram part from https://lppier.github.io/\n",
    "sentence_stream = [simple_preprocess(strip_tags(doc), deacc=True) for doc in documents]\n",
    "bigram = Phrases(sentence_stream, min_count=10)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "\n",
    "indieners = {indiener[-1].lower() for indiener in df['Indiener_persoon'].str.split() if indiener}\n",
    "years = {word for doc in sentence_stream for word in doc if word.startswith('x') and not word.startswith('xin')}\n",
    "manual_stopwords = {'faber', 'lacin', 'kroger', 'iv', 'beschikt', 'die', 'vaststelling', 'lid','vi', 'viii', 'iii', 'iv', 'ii', 'i'}\n",
    "stopwords = indieners | years | manual_stopwords\n",
    "   \n",
    "def bigram_stopword_preprocess(doc):\n",
    "    sentence_stream = simple_preprocess(strip_tags(doc), deacc=True)\n",
    "    sentence_stream = [word for word in sentence_stream if word not in stopwords]\n",
    "    return bigram_phraser[sentence_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'bigram_stopword_preprocess' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-a3e85c2d16e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTop2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deep-learn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbigram_stopword_preprocess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bigram_stopword_preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "model = Top2Vec(documents, speed='deep-learn', tokenizer=bigram_stopword_preprocess, workers=4)\n",
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"data/doc2vec_deep_bigram_enhanced_stopwords\")"
   ]
  },
  {
   "source": [
    "## Reduce topic number to something more manageable"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "metadata": {},
     "execution_count": 244
    }
   ],
   "source": [
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reduced_topics(num_topics):\n",
    "    print(f'performing reduction to {num_topics} topics')\n",
    "    reduced_topics = model.hierarchical_topic_reduction(num_topics)\n",
    "    topic_words, word_scores, topic_nums = model.get_topics(reduced=True)\n",
    "    reduced_topics = tuple(tuple(sorted(t)) for t in reduced_topics)\n",
    "    return reduced_topics, topic_words\n",
    "\n",
    "def find_diff(reduced1, reduced2, reverse=False):\n",
    "    # find the topics that where merged and return their index\n",
    "    if not reverse:\n",
    "        changed = set(reduced1) - set(reduced2)\n",
    "        return [index for index, topic in enumerate(reduced1) if topic in changed]\n",
    "    else:\n",
    "        changed = set(reduced2) - set(reduced1)\n",
    "        return [index for index, topic in enumerate(reduced2) if topic in changed]\n",
    "\n",
    "\n",
    "def print_merge(large, small, num_words=50):\n",
    "    print(f'\\ninspecting difference from {len(large[0])} to {len(small[0])} topics')\n",
    "    print('old topics')\n",
    "    for i in find_diff(large[0],small[0]):\n",
    "        print(large[1][i][:num_words])\n",
    "    print('new topic')\n",
    "    for i in find_diff(large[0],small[0], reverse=True):\n",
    "        print(small[1][i][:num_words])\n",
    "\n",
    "def find_optimal_num_topics():\n",
    "    start = 17\n",
    "    stop = 6\n",
    "    reductions = {i: get_reduced_topics(i) for i in range(start, stop, -1)}\n",
    "    for i in range(start, stop +1,-1):\n",
    "        print(i)\n",
    "        print_merge(reductions[i], reductions[i-1])\n",
    "# find_optimal_num_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a choice to how many topics to reduce\n",
    "num_topics = 16\n",
    "reduced_topics = model.hierarchical_topic_reduction(num_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect number of documents for each topic\n",
    "# topic_sizes, topic_nums = model.get_topic_sizes(reduced=True)\n",
    "# topic_words, word_scores, topic_nums = model.get_topics(reduced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {\n",
    "  0: 'Onderwijs',  \n",
    "  1: 'Buitenlandse zaken',  \n",
    "  2: 'Algemene zaken',  \n",
    "  3: 'Natuur & gaswinning',  \n",
    "  4: 'Landbouw & dierenwelzijn',  \n",
    "  5: 'Zorg',  \n",
    "  6: 'Sociale zaken',  \n",
    "  7: 'Justitie',  \n",
    "  8: 'Pensioenstelsel',  \n",
    "  9: 'Europese Unie',  \n",
    "  10: 'Klimaat & energie',  \n",
    "  11: 'Milieu & regelgeving',  \n",
    "  12: 'Zorg',  \n",
    "  13: 'Openbaar vervoer',  \n",
    "  14: 'Financiele sector',  \n",
    "  15: 'Wonen'\n",
    "  }\n",
    "\n",
    "doc_ids = list(range(len(documents)))\n",
    "topic_nums, topic_score, topic_words, word_scores = model.get_documents_topics(doc_ids,reduced=True)\n",
    "topic_names = [topics[t] for t in topic_nums]\n",
    "assert len(topic_nums) == len(df)\n",
    "df['Topic'] = topic_names\n",
    "topic_nums, topic_score, topic_words, word_scores = model.get_documents_topics(doc_ids,reduced=False)\n",
    "df['Topic_initial'] = topic_nums\n",
    "df['Topic_score'] = topic_score\n",
    "df.sort_values(['Topic_initial', 'Topic_score'], ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "sum(df['Topic_initial']==3)"
   ]
  },
  {
   "source": [
    "## Optional add climate deepdive"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sizes, topic_nums = model.get_topic_sizes(reduced=False)\n",
    "topic_words, word_scores, topic_nums = model.get_topics(reduced=False)\n",
    "climate_idx = [topic_nums for t in topic_nums if t in reduced_topics[10]]\n",
    "climate_subtopics = {128: 'Afhankelijkheid fossiele brandstoffen',\n",
    " 165: 'CO2 reductie',\n",
    " 5: 'Voldoen aan Parijs',\n",
    " 141: 'Electriciteit',\n",
    " 126: 'Groningen',\n",
    " 205: 'Zonnepanelen',\n",
    " 96: 'Energierekening betalen',\n",
    " 236: 'Biomassa',\n",
    " 105: 'Kolencentrales',\n",
    " 239: 'Windturbines - overlast',\n",
    " 29: 'Windturbines - subsidie'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Klimaat'] = df.loc[df['Topic']=='Klimaat & energie', 'Topic_initial'].map(climate_subtopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df[:1000], 'df_including_topics.pickle')\n",
    "save(df, 'df_including_topics_full.pickle')\n"
   ]
  },
  {
   "source": [
    "# Prepare slimmed down versions for production"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "404\n29484\n126\n126\n11448\n"
     ]
    }
   ],
   "source": [
    "df = load(\"df_including_topics_full.pickle\")\n",
    "print(len(df[df['Topic_initial']==3]))\n",
    "print(len(df))\n",
    "df = df[df['Kamer']=='Rutte III']\n",
    "print(len(df[df['Topic_initial']==3]))\n",
    "\n",
    "# important do this only after all row filters have been set\n",
    "df.index = df['Index']\n",
    "\n",
    "print(len(df[df['Topic_initial']==3]))\n",
    "\n",
    "stem_column = [c for c in df.columns if 'Stem_' in c and c != 'Stem_persoon']\n",
    "required_cols = ['Kamer', 'Jaar','Indienende_partij', 'BesluitSoort','BesluitTekst','Topic_initial', 'Topic_score','Indienende_persoon_partij','Partijen_Voor', 'Partijen_Tegen', 'Text']\n",
    "\n",
    "# streamlit has problems with category type: \n",
    "# https://github.com/streamlit/streamlit/issues/47\n",
    "# for col in stem_column + required_cols[:-4]:\n",
    "#     df[col] = df[col].astype('category')\n",
    "\n",
    "df = df[stem_column + required_cols]\n",
    "print(len(df))\n",
    "save(df, \"df_production.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_documents(self, doc_ids):\n",
    "    \"\"\"\n",
    "    Delete documents from current model.\n",
    "    Warning: If document ids were not used in original model, deleting\n",
    "    documents will change the indexes and therefore doc_ids.\n",
    "    The documents will be deleted from the current model without changing\n",
    "    existing document, word and topic vectors. Topic sizes will be updated.\n",
    "    If deleting a large quantity of documents relative to the current model\n",
    "    size a new model should be trained for best results.\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_ids: List of str, int\n",
    "        A unique value per document that is used for referring to documents\n",
    "        in search results.\n",
    "    \"\"\"\n",
    "    # make sure documents exist\n",
    "    self._validate_doc_ids(doc_ids, doc_ids_neg=[])\n",
    "\n",
    "    # update index\n",
    "    if self.documents_indexed:\n",
    "        # delete doc_ids from index\n",
    "        index_ids = [self.doc_id2index_id(doc_id) for doc_id in doc_ids]\n",
    "        for index_id in index_ids:\n",
    "            self.document_index.mark_deleted(index_id)\n",
    "        # update index_id and doc_ids\n",
    "        for doc_id in doc_ids:\n",
    "            self.doc_id2index_id.pop(doc_id)\n",
    "        for index_id in index_ids:\n",
    "            self.index_id2doc_id.pop(index_id)\n",
    "\n",
    "    # get document indexes from ids\n",
    "    doc_indexes = self._get_document_indexes(doc_ids)\n",
    "\n",
    "    # delete documents\n",
    "    if self.documents is not None:\n",
    "        self.documents = np.delete(self.documents, doc_indexes, 0)\n",
    "\n",
    "    # delete document ids\n",
    "    if self.document_ids is not None:\n",
    "        for doc_id in doc_ids:\n",
    "            self.doc_id2index.pop(doc_id)\n",
    "        keys = list(self.doc_id2index.keys())\n",
    "        self.document_ids = np.array(keys)\n",
    "        values = list(range(0, len(self.doc_id2index.values())))\n",
    "        self.doc_id2index = dict(zip(keys, values))\n",
    "\n",
    "    # delete document vectors\n",
    "    self._set_document_vectors(np.delete(self._get_document_vectors(norm=False), doc_indexes, 0))\n",
    "\n",
    "    if self.embedding_model == 'doc2vec':\n",
    "        num_docs = len(doc_indexes)\n",
    "        self.model.docvecs.count -= num_docs\n",
    "        self.model.docvecs.max_rawint -= num_docs\n",
    "        self.model.docvecs.vectors_docs_norm = None\n",
    "        self.model.docvecs.init_sims()\n",
    "\n",
    "    # update topics\n",
    "    # self._unassign_documents_from_topic(doc_indexes, hierarchy=False)\n",
    "\n",
    "    if self.hierarchy is not None:\n",
    "        self._unassign_documents_from_topic(doc_indexes, hierarchy=True)\n",
    "\n",
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "model.delete_documents = delete_documents\n",
    "df = load(\"df_including_topics_full.pickle\")\n",
    "model.delete_documents(model,list(range(len(df))))\n",
    "model.delete_documents = 1\n",
    "model.save(\"data/doc2vec_production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 5, 96, 25, 24], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords= ['klimaat'] , num_topics=4)\n",
    "topic_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Backup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary = load(\"df_production.pickle\")\n",
    "necessary_idx = set(necessary.index)\n",
    "\n",
    "full = load(\"df_including_topics_full.pickle\")\n",
    "full_idx = set(full['Index'])\n",
    "\n",
    "redundant_idx = full_idx - necessary_idx\n",
    "assert len(necessary_idx) + len(redundant_idx) == len(full_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0], dtype=int64),\n",
       " array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246],\n",
       "       dtype=int64))"
      ]
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "model.get_topic_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([22728,  9991,  5958,  2969,  6021,  8364, 19787,  8385,  8357,\n",
       "       16140])"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num =3, num_docs= 100)\n",
    "document_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove_3 = full[(full['Topic_initial']==3)&(full['Index'].isin(redundant_idx))]['Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "len(list(to_remove_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[531 458 445 404 401 350 347 333 326 312 309 308 301 287 285 278 276 273\n",
      " 270 265 263 257 255 240 236 230 230 228 227 222 220 211 205 205 203 199\n",
      " 197 195 193 192 190 182 181 180 175 174 171 169 166 165 164 163 161 159\n",
      " 159 159 158 156 155 154 154 151 151 150 150 148 147 143 141 141 136 134\n",
      " 133 133 132 131 129 127 126 125 124 124 122 121 120 118 118 118 117 116\n",
      " 114 113 112 110 109 108 108 107 107 105 105 103 103 103 102 100  99  98\n",
      "  98  97  96  96  96  96  96  96  95  95  95  94  93  91  91  91  89  88\n",
      "  88  88  87  87  86  86  86  85  85  84  84  84  83  83  81  81  81  81\n",
      "  80  80  80  80  79  79  79  79  79  79  78  77  76  75  75  75  75  74\n",
      "  73  72  72  71  71  70  70  69  69  68  68  68  67  67  67  67  65  65\n",
      "  65  64  63  62  62  61  60  60  59  59  58  58  57  57  57  56  56  56\n",
      "  56  55  55  55  54  54  53  52  52  52  52  52  50  50  49  48  47  47\n",
      "  46  46  46  45  45  45  45  45  45  44  44  43  43  42  41  41  40  40\n",
      "  39  38  38  38  37  36  35  34  34  33  33  32  23]\n",
      "[531 458 445 401 350 347 333 326 312 309 308 301 287 285 278 276 273 270\n",
      " 265 263 257 255 240 236 230 230 228 227 222 220 211 205 205 203 199 197\n",
      " 195 193 192 190 182 181 180 175 174 171 169 166 165 164 163 161 159 159\n",
      " 159 158 156 155 154 154 151 151 150 150 148 147 143 141 141 136 134 133\n",
      " 133 132 131 129 127 126 126 125 124 124 122 121 120 118 118 118 117 116\n",
      " 114 113 112 110 109 108 108 107 107 105 105 103 103 103 102 100  99  98\n",
      "  98  97  96  96  96  96  96  96  95  95  95  94  93  91  91  91  89  88\n",
      "  88  88  87  87  86  86  86  85  85  84  84  84  83  83  81  81  81  81\n",
      "  80  80  80  80  79  79  79  79  79  79  78  77  76  75  75  75  75  74\n",
      "  73  72  72  71  71  70  70  69  69  68  68  68  67  67  67  67  65  65\n",
      "  65  64  63  62  62  61  60  60  59  59  58  58  57  57  57  56  56  56\n",
      "  56  55  55  55  54  54  53  52  52  52  52  52  50  50  49  48  47  47\n",
      "  46  46  46  45  45  45  45  45  45  44  44  43  43  42  41  41  40  40\n",
      "  39  38  38  38  37  36  35  34  34  33  33  32  23]\n"
     ]
    }
   ],
   "source": [
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "print(model.get_topic_sizes()[0])\n",
    "model.delete_documents(list(to_remove_3))\n",
    "print(model.get_topic_sizes()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Invalid number of documents: original topic 3 only has 0 documents.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-1cb2ef9b984a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdocument_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_documents_by_topic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_num\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m404\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_documents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36msearch_documents_by_topic\u001b[1;34m(self, topic_num, num_docs, return_documents, reduced)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1737\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_topic_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1738\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_topic_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1740\u001b[0m             \u001b[0mtopic_document_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc_top\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtopic_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\top2vec\\Top2Vec.py\u001b[0m in \u001b[0;36m_validate_topic_search\u001b[1;34m(self, topic_num, num_docs, reduced)\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnum_docs\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopic_sizes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                 raise ValueError(f\"Invalid number of documents: original topic {topic_num}\"\n\u001b[0m\u001b[0;32m    872\u001b[0m                                  f\" only has {self.topic_sizes[topic_num]} documents.\")\n\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid number of documents: original topic 3 only has 0 documents."
     ]
    }
   ],
   "source": [
    "# model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "document_scores, document_ids = model.search_documents_by_topic(topic_num =3, num_docs= 404,return_documents=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "29484"
      ]
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "model = Top2Vec.load(\"data/doc2vec_deep_bigram_enhanced_stopwords\")\n",
    "sums, _ = model.get_topic_sizes()\n",
    "sum(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2\\nTweede Kamer der Staten-Generaal\\nVergaderjaar 2019–2020 \\n35 420 Noodpakket banen en economie \\nNr. 66   MOTIE VAN DE LEDEN BAUDET EN VAN HAGA \\nVoorgesteld 28 mei 2020 \\nDe Kamer, \\ngehoord de beraadslaging, \\nconstaterende dat in 2019 de Klimaatwet in werking is getreden; \\noverwegende dat de kosten van het Nederlandse klimaatbeleid naar \\nverwachting zullen oplopen tot in totaal 1.000 miljard euro; \\noverwegende dat het Nederlandse klimaatbeleid, zelfs als de theorie over \\nopwarming van de aarde door toedoen van de mens zou kloppen, die \\nopwarming met hoogstens 0,00007 graden zou beperken; \\noverwegende dat de kosten van het klimaatbeleid in geen enkele \\nverhouding staan tot de opbrengsten; \\nroept de regering op, om het klimaatbeleid te staken en het geld dat \\nhierdoor bespaard wordt in te zetten om de klappen van de coronacrisis \\nop te vangen, \\nen gaat over tot de orde van de dag. \\nBaudet \\nVan Haga\\n \\n \\n \\n \\nkst-35420-66\\nISSN 0921 - 7371\\n’s-Gravenhage 2020 Tweede Kamer, vergaderjaar 2019–2020, 35 420, nr. 66'"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "len(necessary[necessary['Topic_initial']==3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "404-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}